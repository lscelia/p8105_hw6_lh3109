---
title: "p8105_hw6_lh3109"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)

set.seed(1)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_color_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

## Propose a regression model
### Load and tidy data
```{r, message=FALSE}
bw_df = 
  #import data
  read_csv("./data/birthweight.csv") %>% 
  #remove possible na values
  drop_na() %>% 
  #factor appropriate numeric variables
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )
```
This data set contains 4342 observations with 22 variables. It does not contains NA values.

### Variable selection
```{r}
#test model by considering all variables as predictors
mult_fit = lm(bwt ~ . , data = bw_df)

#using stepwise backward function to select predictors
step(mult_fit, direction = 'backward')
```

As shown in the output of the step() function, the selected predictors are babysex, bhead, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt, and smoken.


### Fit model
Apply the selected predictors to the model 
```{r}
#fit the model
fit = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = bw_df)

#check the results
summary(fit)
```

### Plot residuals vs. fitted values
```{r}
bw_df %>% 
  modelr::add_residuals(fit) %>% 
  modelr::add_predictions(fit) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  labs(title = "Model Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals")
```


As shown in the plot, with fitted value below 2000 or above 4000, the residuals do not bounce randomly around the 0 line, suggesting that the data point in this range do not fall on the estimated regression line. This model is not optimal for data outside of the 2000-4000 range for fitted values and there are many outliers However, when fitted values are around 3000, many data points have residuals equals to 0 and they falls directly on the estimated regression line. 


## Compare the model with two others using CV

### Set up training and test subsets
```{r}
birthweight_cv = 
  crossv_mc(bw_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

### Fit the three models
```{r, warning=FALSE}
birthweight_cv = 
  birthweight_cv %>% 
  mutate(
    mod = map(.x = train, 
              ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + 
                mheight + mrace + parity + ppwt + smoken, 
              data = .x)),
    mod_effects = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod_interactions = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + 
                                  bhead * blength + bhead * babysex + 
                                  blength * babysex + bhead * blength * babysex, 
                                data = .x))
  ) %>% 
  mutate(
    #map 2 things, show double value
    rmse_model_proposed = map2_dbl(.x = mod, .y = test, ~rmse(model = .x, data = .y )),
    rmse_model_main_effects = map2_dbl(.x = mod_effects, .y = test, ~rmse(model = .x, data = .y )),
    rmse_model_all_interactions = map2_dbl(.x = mod_interactions, .y = test, ~rmse(model = .x, data = .y ))
  )
```

### Look at the outputs
```{r, message=FALSE}
birthweight_cv %>% 
  select(.id, starts_with("rmse")) %>% 
  pivot_longer(
    rmse_model_proposed:rmse_model_all_interactions,
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_model_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot() +
    labs(title = "Model Comparisons",
       x = "Models",
       y = "RMSE Values")
```

As shown in the plot, the proposed model has much lower rmse values than the model considering length at birth and gestational age as predictors(main_effects). It also has lower rmse values than the model considering head circumference, length, sex and all interactions of these three variables as predictors(all_interactions). These indicates that the proposed model is better than the two model compared.



# Prolem 2

## Load data
```{r, message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


## Bootstrapping

### Write a function for one boostrap sample analysis
```{r}
boot_sample = function(mod) {
  #find adjusted r square of the estimates
  r_squared = 
    mod %>% 
    broom::glance() %>% 
    pull(adj.r.squared)
  
  #tidy the results of the model
  result = 
    mod %>% 
    broom::tidy()
  
  #pull estimates values for further analysis
  betas = 
    result %>% 
    pull(estimate)
  
  #return a tibble
  tibble(result, r_squared, log_betas = log(betas[1] * betas[2]))
}
```

### Boostrap sample for multiple times
```{r}
weather_results = 
  weather_df %>% 
  bootstrap(n = 5000, id = "strap_number") %>% 
    mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, boot_sample)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)
```


### Plot distribution of estimates
```{r}
weather_results %>% 
  filter(term == "tmin") %>% 
  ggplot(aes(estimate)) +
  geom_density() + 
  labs(title = "Distribution of Estimates",
       x = "Estimate",
       y = "Density")  
```

This plot shows the distribution of the estimates of tmin after repeating bootstrapping the sample multiple times. The peak is almost symmetric, indicating that the distribution is approximately normal.  


### Plot distribution of R^2
```{r}
weather_results %>% 
  ggplot(aes(x = r_squared)) +
  geom_density() + 
  labs(title = "Distribution of R^2 of Estimates",
       x = "R^2",
       y = "Density")
```

The plot of the distribution of R^2 of the estimates shown above is relatively symmetric, suggesting that this distribution is approximately normal.



### Plot distribution of log(beta0*beta1)
```{r}
weather_results %>% 
  ggplot(aes(x = log_betas)) +
  geom_density() + 
  labs(title = "Distribution of Log(beta_0 * beta_1)",
       x = "Log(beta_0 * beta_1)",
       y = "Density")
```

The plot of the distribution of log(beta_0 * beta_1) shown above is relatively symmetric, suggesting that this distribution is approximately normal.



### Find 95% CI for R^2
```{r}
weather_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(r_squared, 0.025), 
    ci_upper = quantile(r_squared, 0.975))
```
As shown in the output, the 95% CI for the R^2 is (0.893, 0.927).

### Find 95% CI for log(beta0*beta1)
```{r}
weather_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(log_betas, 0.025), 
    ci_upper = quantile(log_betas, 0.975))
```
As shown in the output, the 95% CI for the log(beta0*beta1) is (1.97, 2.06).

